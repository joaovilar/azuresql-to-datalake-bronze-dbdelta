{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9dcf88b-d871-4e81-96e0-546eb1aa2126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Conexão com o Azure SQL Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b25b424e-8a48-44b2-ab81-97a4f6ed6470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuração de conexão com o Azure SQL Server\n",
    "driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "database_host = \"azure-sql-dev-1.database.windows.net\"\n",
    "database_port = \"1433\"  # Porta padrão\n",
    "database_name = \"store\"\n",
    "table_name = \"SalesLT.Customer\"  # Nome completo da tabela\n",
    "user = \"sqladmin\"\n",
    "password = \"153759Sql\"\n",
    "\n",
    "url = f\"jdbc:sqlserver://{database_host}:{database_port};database={database_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2846f068-b45a-43ed-a84b-4c47de3f18a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Conexão com o Azure Datalake Storage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04a42b4-bcb7-47bd-88d9-2020a7b52913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuração de autenticação para o Data Lake\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": \"a446480f-7288-414c-be4d-5ec767e18ab0\",\n",
    "    \"fs.azure.account.oauth2.client.secret\": \"YBH8Q~7NZ0MXj0ROJBHyw.PhtAaR2pocqAD~Ycnb\",\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/cff92cd3-d5ac-409a-989e-2cbdb7822bb3/oauth2/token\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "306f632f-96e4-4268-a6de-dbee7f31c6fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A pasta /mnt/datalakebikestore/sqldatabase já está montada.\n"
     ]
    }
   ],
   "source": [
    "# Montagem do Data Lake no Databricks\n",
    "container_name = \"sqldatabase\"\n",
    "storage_account_name = \"datalakebikestore\"\n",
    "mount_point = f\"/mnt/{storage_account_name}/{container_name}\"\n",
    "\n",
    "# Verificar se o ponto de montagem já existe\n",
    "if not any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    try:\n",
    "        dbutils.fs.mount(\n",
    "            source=f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/\",\n",
    "            mount_point=mount_point,\n",
    "            extra_configs=configs\n",
    "        )\n",
    "        print(f\"Montagem bem-sucedida: {mount_point}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na montagem: {e}\")\n",
    "else:\n",
    "    print(f\"A pasta {mount_point} já está montada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c923ea8-493f-488a-96f0-53e3c2a44335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelas a processar: ['Customer', 'Product', 'ProductCategory', 'ProductModel', 'Address', 'SalesOrderHeader', 'SalesOrderDetail']\n"
     ]
    }
   ],
   "source": [
    "# Lista das tabelas\n",
    "schema = \"SalesLT\"\n",
    "tables = [\"Customer\", \"Product\", \"ProductCategory\", \"ProductModel\", \"Address\", \"SalesOrderHeader\", \"SalesOrderDetail\"]\n",
    "\n",
    "# Caminho base no Data Lake\n",
    "bronze_path = f\"{mount_point}/bronze\"\n",
    "print(f\"Tabelas a processar: {tables}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99dd013-ecb3-4f52-83df-ea8e1299579b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "def process_and_save_table(schema, table_name, mount_point):\n",
    "    full_table_name = f\"{schema}.{table_name}\"\n",
    "    \n",
    "    # Leitura da tabela do SQL Server\n",
    "    try:\n",
    "        df = (spark.read\n",
    "            .format(\"jdbc\")\n",
    "            .option(\"driver\", driver)\n",
    "            .option(\"url\", url)\n",
    "            .option(\"dbtable\", full_table_name)\n",
    "            .option(\"user\", user)\n",
    "            .option(\"password\", password)\n",
    "            .load()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Erro ao ler a tabela {full_table_name}: {e}\")\n",
    "    \n",
    "    # Adicionar a coluna com a data do dia corrente\n",
    "    df_with_date = df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "    \n",
    "    # Definir o caminho de destino no Data Lake\n",
    "    destination_path = f\"{mount_point}/bronze/{table_name}\"\n",
    "    \n",
    "    # Salvar os dados no Data Lake em formato Parquet\n",
    "    try:\n",
    "        df_with_date.write \\\n",
    "            .format(\"parquet\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save(destination_path)\n",
    "        \n",
    "        print(f\"Tabela {full_table_name} salva com sucesso no caminho {destination_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Erro ao salvar a tabela {full_table_name}: {e}\")\n",
    "    \n",
    "    return destination_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a3d685-07fe-4dad-aef0-a70a67321ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela SalesLT.Customer salva com sucesso no caminho /mnt/datalakebikestore/sqldatabase/bronze/Customer\nArquivos salvos no caminho: /mnt/datalakebikestore/sqldatabase/bronze/Customer\n_SUCCESS\n_committed_3737451322271888108\n_started_3737451322271888108\npart-00000-tid-3737451322271888108-d9f83ffb-ba3c-43f6-b415-6a4110c1b5aa-253-1-c000.snappy.parquet\nTabela SalesLT.Product salva com sucesso no caminho /mnt/datalakebikestore/sqldatabase/bronze/Product\nArquivos salvos no caminho: /mnt/datalakebikestore/sqldatabase/bronze/Product\n_SUCCESS\n_committed_6516409335988904560\n_started_6516409335988904560\npart-00000-tid-6516409335988904560-cfbc9b83-23be-4f8e-a715-77c87c8644cd-254-1-c000.snappy.parquet\nTabela SalesLT.ProductCategory salva com sucesso no caminho /mnt/datalakebikestore/sqldatabase/bronze/ProductCategory\nArquivos salvos no caminho: /mnt/datalakebikestore/sqldatabase/bronze/ProductCategory\n_SUCCESS\n_committed_4579912753772404239\n_started_4579912753772404239\npart-00000-tid-4579912753772404239-8fe38226-2589-422c-b3a3-912a090f1462-255-1-c000.snappy.parquet\nTabela SalesLT.ProductModel salva com sucesso no caminho /mnt/datalakebikestore/sqldatabase/bronze/ProductModel\nArquivos salvos no caminho: /mnt/datalakebikestore/sqldatabase/bronze/ProductModel\n_SUCCESS\n_committed_4262817634917916244\n_started_4262817634917916244\npart-00000-tid-4262817634917916244-da3f1291-3f3f-4c17-9db3-f5aec1290440-256-1-c000.snappy.parquet\nTabela SalesLT.Address salva com sucesso no caminho /mnt/datalakebikestore/sqldatabase/bronze/Address\nArquivos salvos no caminho: /mnt/datalakebikestore/sqldatabase/bronze/Address\n_SUCCESS\n_committed_7387843983232096129\n_started_7387843983232096129\npart-00000-tid-7387843983232096129-6e8d156a-8b70-4d31-9f83-8ec1691bd63f-257-1-c000.snappy.parquet\nTabela SalesLT.SalesOrderHeader salva com sucesso no caminho /mnt/datalakebikestore/sqldatabase/bronze/SalesOrderHeader\nArquivos salvos no caminho: /mnt/datalakebikestore/sqldatabase/bronze/SalesOrderHeader\n_SUCCESS\n_committed_3368637419660565332\n_started_3368637419660565332\npart-00000-tid-3368637419660565332-2195a8ee-0fda-4e4a-8672-0398740c6be2-258-1-c000.snappy.parquet\nTabela SalesLT.SalesOrderDetail salva com sucesso no caminho /mnt/datalakebikestore/sqldatabase/bronze/SalesOrderDetail\nArquivos salvos no caminho: /mnt/datalakebikestore/sqldatabase/bronze/SalesOrderDetail\n_SUCCESS\n_committed_3336498906252210294\n_started_3336498906252210294\npart-00000-tid-3336498906252210294-b7c1f63a-aa68-4004-8753-61b14bd3d6a2-259-1-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Processar todas as tabelas\n",
    "for table in tables:\n",
    "    try:\n",
    "        destination_path = process_and_save_table(schema, table, mount_point)\n",
    "        \n",
    "        # Listar os arquivos salvos para verificar\n",
    "        print(f\"Arquivos salvos no caminho: {destination_path}\")\n",
    "        files = dbutils.fs.ls(destination_path)\n",
    "        for file in files:\n",
    "            print(file.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar a tabela {table}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c9bc581-604f-4870-a33a-f60a944279af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pastas criadas no Data Lake:\nErro ao acessar a pasta para a tabela Customer: An error occurred while calling o451.ls.\n: java.io.FileNotFoundException: /mnt/datalakebikestore/sqldatabase/bronze/SalesLT_Customer\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:274)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:242)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:215)\n\tat sun.reflect.GeneratedMethodAccessor755.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\nErro ao acessar a pasta para a tabela Product: An error occurred while calling o451.ls.\n: java.io.FileNotFoundException: /mnt/datalakebikestore/sqldatabase/bronze/SalesLT_Product\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:274)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:242)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:215)\n\tat sun.reflect.GeneratedMethodAccessor755.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\nErro ao acessar a pasta para a tabela Address: An error occurred while calling o451.ls.\n: java.io.FileNotFoundException: /mnt/datalakebikestore/sqldatabase/bronze/SalesLT_Address\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:274)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:242)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:215)\n\tat sun.reflect.GeneratedMethodAccessor755.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\nErro ao acessar a pasta para a tabela SalesOrderHeader: An error occurred while calling o451.ls.\n: java.io.FileNotFoundException: /mnt/datalakebikestore/sqldatabase/bronze/SalesLT_SalesOrderHeader\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:274)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:242)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:215)\n\tat sun.reflect.GeneratedMethodAccessor755.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\nErro ao acessar a pasta para a tabela SalesOrderDetail: An error occurred while calling o451.ls.\n: java.io.FileNotFoundException: /mnt/datalakebikestore/sqldatabase/bronze/SalesLT_SalesOrderDetail\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:121)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:161)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:274)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:243)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:144)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:242)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:215)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:215)\n\tat sun.reflect.GeneratedMethodAccessor755.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    }
   ],
   "source": [
    "# Lista de tabelas no schema SalesLT\n",
    "tables = [\"Customer\", \"Product\", \"Address\", \"SalesOrderHeader\", \"SalesOrderDetail\"]\n",
    "\n",
    "# Listar pastas criadas para cada tabela no Data Lake\n",
    "print(\"Pastas criadas no Data Lake:\")\n",
    "for table in tables:\n",
    "    # Caminho da pasta referente à tabela no Data Lake\n",
    "    table_path = f\"{mount_point}/bronze/SalesLT_{table}\"\n",
    "    \n",
    "    try:\n",
    "        # Listar o conteúdo da pasta\n",
    "        items = dbutils.fs.ls(table_path)\n",
    "        \n",
    "        # Verificar se a pasta existe e exibir seu conteúdo\n",
    "        print(f\"Tabela: {table}\")\n",
    "        print(f\"Pasta: {table_path}\")\n",
    "        print(\"Conteúdo da pasta:\")\n",
    "        for item in items:\n",
    "            print(f\"  - {item.name}\")\n",
    "    except Exception as e:\n",
    "        # Caso a pasta não exista ou haja erro\n",
    "        print(f\"Erro ao acessar a pasta para a tabela {table}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5312f126-d07e-4186-b19e-e4c823af6c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[157]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Criar o banco de dados, caso não exista\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS sales_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90cf945f-7e7b-4d43-970a-0c3b6ab47acc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Carrega as tabelas como delta no banco de dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce8813e7-ec15-402f-a422-2b184cc9827f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabela Customer salva com sucesso no banco de dados sales_data como Customer_bronze\nTabela Product salva com sucesso no banco de dados sales_data como Product_bronze\nTabela ProductCategory salva com sucesso no banco de dados sales_data como ProductCategory_bronze\nTabela ProductModel salva com sucesso no banco de dados sales_data como ProductModel_bronze\nTabela Address salva com sucesso no banco de dados sales_data como Address_bronze\nTabela SalesOrderHeader salva com sucesso no banco de dados sales_data como SalesOrderHeader_bronze\nTabela SalesOrderDetail salva com sucesso no banco de dados sales_data como SalesOrderDetail_bronze\n"
     ]
    }
   ],
   "source": [
    "# Lista de tabelas\n",
    "tables = [\"Customer\",\"Product\", \"ProductCategory\", \"ProductModel\", \"Address\", \"SalesOrderHeader\", \"SalesOrderDetail\"]\n",
    "\n",
    "# Iterar sobre a lista de tabelas\n",
    "for table in tables:\n",
    "    # Carregar a tabela do SQL Server\n",
    "    full_table_name = f\"SalesLT.{table}\"\n",
    "    df = (spark.read\n",
    "          .format(\"jdbc\")\n",
    "          .option(\"driver\", driver)\n",
    "          .option(\"url\", url)\n",
    "          .option(\"dbtable\", full_table_name)\n",
    "          .option(\"user\", user)\n",
    "          .option(\"password\", password)\n",
    "          .load()\n",
    "    )\n",
    "    \n",
    "    # Adicionar a coluna com a data do dia corrente\n",
    "    df_with_date = df.withColumn(\"ingestion_date\", current_timestamp())\n",
    "    \n",
    "    # Criar a tabela no banco de dados sales_data\n",
    "    df_with_date.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(f\"sales_data.{table}_bronze\")\n",
    "    \n",
    "    print(f\"Tabela {table} salva com sucesso no banco de dados sales_data como {table}_bronze\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39f0d69f-25cc-4eb8-83ec-7ddeb5492b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Listando todas as tabelas criadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b076413b-6b73-439c-88f4-cdfdb66b9c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database</th><th>tableName</th><th>isTemporary</th></tr></thead><tbody><tr><td>sales_data</td><td>address_bronze</td><td>false</td></tr><tr><td>sales_data</td><td>customer_bronze</td><td>false</td></tr><tr><td>sales_data</td><td>product_bronze</td><td>false</td></tr><tr><td>sales_data</td><td>productcategory_bronze</td><td>false</td></tr><tr><td>sales_data</td><td>productmodel_bronze</td><td>false</td></tr><tr><td>sales_data</td><td>salesorderdetail_bronze</td><td>false</td></tr><tr><td>sales_data</td><td>salesorderheader_bronze</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "sales_data",
         "address_bronze",
         false
        ],
        [
         "sales_data",
         "customer_bronze",
         false
        ],
        [
         "sales_data",
         "product_bronze",
         false
        ],
        [
         "sales_data",
         "productcategory_bronze",
         false
        ],
        [
         "sales_data",
         "productmodel_bronze",
         false
        ],
        [
         "sales_data",
         "salesorderdetail_bronze",
         false
        ],
        [
         "sales_data",
         "salesorderheader_bronze",
         false
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "tableName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isTemporary",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Consultar todas as tabelas do banco de dados \"sales_data\"\n",
    "SHOW TABLES IN sales_data;\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "618d729b-faae-479c-87f8-2c0142d84253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>ProductID</td><td>int</td><td>null</td></tr><tr><td>Name</td><td>string</td><td>null</td></tr><tr><td>ProductNumber</td><td>string</td><td>null</td></tr><tr><td>Color</td><td>string</td><td>null</td></tr><tr><td>StandardCost</td><td>decimal(19,4)</td><td>null</td></tr><tr><td>ListPrice</td><td>decimal(19,4)</td><td>null</td></tr><tr><td>Size</td><td>string</td><td>null</td></tr><tr><td>Weight</td><td>decimal(8,2)</td><td>null</td></tr><tr><td>ProductCategoryID</td><td>int</td><td>null</td></tr><tr><td>ProductModelID</td><td>int</td><td>null</td></tr><tr><td>SellStartDate</td><td>timestamp</td><td>null</td></tr><tr><td>SellEndDate</td><td>timestamp</td><td>null</td></tr><tr><td>DiscontinuedDate</td><td>timestamp</td><td>null</td></tr><tr><td>ThumbNailPhoto</td><td>binary</td><td>null</td></tr><tr><td>ThumbnailPhotoFileName</td><td>string</td><td>null</td></tr><tr><td>rowguid</td><td>string</td><td>null</td></tr><tr><td>ModifiedDate</td><td>timestamp</td><td>null</td></tr><tr><td>ingestion_date</td><td>date</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ProductID",
         "int",
         null
        ],
        [
         "Name",
         "string",
         null
        ],
        [
         "ProductNumber",
         "string",
         null
        ],
        [
         "Color",
         "string",
         null
        ],
        [
         "StandardCost",
         "decimal(19,4)",
         null
        ],
        [
         "ListPrice",
         "decimal(19,4)",
         null
        ],
        [
         "Size",
         "string",
         null
        ],
        [
         "Weight",
         "decimal(8,2)",
         null
        ],
        [
         "ProductCategoryID",
         "int",
         null
        ],
        [
         "ProductModelID",
         "int",
         null
        ],
        [
         "SellStartDate",
         "timestamp",
         null
        ],
        [
         "SellEndDate",
         "timestamp",
         null
        ],
        [
         "DiscontinuedDate",
         "timestamp",
         null
        ],
        [
         "ThumbNailPhoto",
         "binary",
         null
        ],
        [
         "ThumbnailPhotoFileName",
         "string",
         null
        ],
        [
         "rowguid",
         "string",
         null
        ],
        [
         "ModifiedDate",
         "timestamp",
         null
        ],
        [
         "ingestion_date",
         "date",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"comment\":\"name of the column\"}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"data type of the column\"}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{\"comment\":\"comment of the column\"}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# DESCRIBE sales_data.product_bronze;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 427584244151981,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "azuresql-to-datalake-bronze-dbdelta",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
